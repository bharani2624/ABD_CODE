Our method provides real-time navigation support for visually impaired people by seamlessly integrating computer vision and Internet of Things technology with a voice-based user interface. Hands-free navigation is made possible by user interaction through NLP and speech recognition. Accurate obstacle detection and depth measurement are guaranteed when using ultrasonic sensors like the HC-SR04 in conjunction with the YOLO v9 object detection model. The object detection model and sensor data processing are managed by the Raspberry Pi, which acts as the hub of the Internet of Things. Mapping libraries such as Folium allow for offline navigation, and CMU Sphinx allows for real-time voice-guided navigation.